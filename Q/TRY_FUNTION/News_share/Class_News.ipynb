{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from collections import Counter\n",
    "from spacy import displacy \n",
    "import locationtagger\n",
    "from geopy.geocoders import Nominatim\n",
    "from functools import partial\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import numpy\n",
    "import sqlite3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class News():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.Url_webdriver = r'C:\\Users\\JourneyQ\\OneDrive - kmutnb.ac.th\\Desktop\\Quick_file\\year_2-S_2\\Project_sofedev_2\\Schap\\chromedriver.exe'\n",
    "\n",
    "    def Set_up_WebScraping(self,url):\n",
    "    \n",
    "        driver = webdriver.Chrome(self.Url_webdriver)\n",
    "        driver.get(url)\n",
    "\n",
    "        for i in range(45):\n",
    "            driver.execute_script(\"scrollBy(0,+500);\")\n",
    "            time.sleep(1)\n",
    "        \n",
    "        return driver.page_source\n",
    "    \n",
    "    def YahooUpdateNews(self,share):\n",
    "\n",
    "        url = requests.get(f\"https://finance.yahoo.com/quote/{share}\")\n",
    "        chest,tag_share_news  = [],{}\n",
    "        soup = BeautifulSoup(url.text, 'html')\n",
    "\n",
    "        for div_all in soup.find_all('div', {'id': 'quoteNewsStream-0-Stream'}):\n",
    "\n",
    "            for li in div_all.find_all('li', {'class': 'js-stream-content Pos(r)'}):\n",
    "\n",
    "                for l in li.find_all('div', {'class': 'Py(14px) Pos(r)'}):\n",
    "\n",
    "                    df,st,share_tag  = {},'',{}\n",
    "                    \n",
    "                    tag_a = l.find('a')\n",
    "                    \n",
    "                    # Img\n",
    "                    tag_img = l.find('img')\n",
    "                    if str(tag_img) != 'None': url_img = l.find('img')['src']\n",
    "                    else : url_img = 'https://www.lifewire.com/thmb/yx5oJUJ4fA1TQ0h0pl9FM7Kc4Fo=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/yahoo-logo-2019-879b7bed612d4bbc97065dce2a0f2d73.png'\n",
    "\n",
    "                    # Get Content -----------------------------------------------------\n",
    "                    \n",
    "                    soup_content = BeautifulSoup((requests.get('https://finance.yahoo.com/' + tag_a['href'])).text, 'html')\n",
    "\n",
    "                    for li_ in soup_content.find_all('p'):\n",
    "                        text = li_.text\n",
    "                        if text != '':st += li_.text + '\\n'\n",
    "\n",
    "                    try :        \n",
    "\n",
    "                        df['Date'],df['Title'],df['Description'],df['Img'],df['Link'],df['Source'],df['Content']  = datetime.datetime.strptime(soup_content.find('time')['datetime'], '%Y-%m-%dT%H:%M:%S.%f%z').strftime('%d/%m/%Y'),tag_a.text,l.find('p').text,url_img,'https://finance.yahoo.com/' + tag_a['href'],'yahoo',st\n",
    "                        share_news = [i.text for i in soup_content.find_all('div', {'class': 'xray-card-row-title'})]\n",
    "                        tag_share_news[df['Title']] = share_news\n",
    "                        print(share_news)\n",
    "                        chest.append(df)\n",
    "                    except TypeError:\n",
    "                        pass\n",
    "\n",
    "        return chest,tag_share_news\n",
    "    \n",
    "    def Kaohoon_news(self,triker):\n",
    "\n",
    "        chest,I,tag_share_news  = [],0,{}\n",
    "\n",
    "        url = requests.get(f\"https://www.kaohoon.com/?s={triker}\")\n",
    "        soup = BeautifulSoup(url.text, 'html')\n",
    "        for ultag in soup.find_all('ul', {'class': 'posts-items'}):\n",
    "\n",
    "            for li in ultag.find_all('li'):\n",
    "\n",
    "                df,st  = {},''\n",
    "                text = (li.text).split('\\n')\n",
    "\n",
    "                df['Date'],df['Title'],df['Description'],df['Img'],df['Link'],df['Source'] = text[3],text[4],text[5],li.find('img')['src'],li.find('a')['href'],'kaohoon'\n",
    "\n",
    "                soup = BeautifulSoup((requests.get(f\"{df['Link']}\")).text, 'html')\n",
    "\n",
    "                div = soup.find('div', {'class': 'entry-content entry clearfix'})\n",
    "\n",
    "                if str(div) == 'None' : return {}\n",
    "                for li in div.find_all('p'):\n",
    "                    text = li.text\n",
    "                    if text != '':\n",
    "                        st += li.text + '\\n'\n",
    "                \n",
    "                df['Content'] = st\n",
    "                tag_share_news[df['Title']] = triker\n",
    "\n",
    "                if st != '': chest.append(df)\n",
    "                \n",
    "        return chest,tag_share_news\n",
    "    \n",
    "    def get_id_News(title):\n",
    "        # connect to the database\n",
    "        conn = sqlite3.connect(r'C:\\Users\\JourneyQ\\OneDrive - kmutnb.ac.th\\Desktop\\Quick_file\\year_2-S_2\\Project_sofedev_2\\Manage_Share\\Database_convert\\share_V2.sqlite')\n",
    "        c = conn.cursor()\n",
    "        id = ''\n",
    "        # search for words in the table\n",
    "        c.execute(f\"\"\"SELECT * FROM News WHERE title=\"{title}\" \"\"\")\n",
    "        results = c.fetchall()\n",
    "\n",
    "        if results != []: id = str(results[0][0])\n",
    "            \n",
    "        else : id = 'None'\n",
    "\n",
    "        # close the database connection\n",
    "        conn.close()\n",
    "\n",
    "        return id\n",
    "        \n",
    "    def add_data_to_db(self,table_name, column ,data):\n",
    "\n",
    "        conn = sqlite3.connect(r'C:\\Users\\JourneyQ\\OneDrive - kmutnb.ac.th\\Desktop\\Quick_file\\year_2-S_2\\Project_sofedev_2\\Manage_Share\\Database_convert\\share_V2.sqlite')\n",
    "        c = conn.cursor()\n",
    "\n",
    "        # Create a string with placeholders for each value in the data tuple\n",
    "        placeholders = \",\".join([\"?\" for _ in data])\n",
    "\n",
    "        # Construct the SQL query string\n",
    "        query_string = f\"INSERT INTO {table_name} ({column}) VALUES ({placeholders})\"\n",
    "\n",
    "        # Execute the query and commit the changes\n",
    "        c.execute(query_string, data)\n",
    "        conn.commit()\n",
    "\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "\n",
    "    def save_into_db(self,table,dic):\n",
    "\n",
    "        df = pd.DataFrame(dic)\n",
    "        con = sqlite3.connect(r'C:\\Users\\JourneyQ\\OneDrive - kmutnb.ac.th\\Desktop\\Quick_file\\year_2-S_2\\Project_sofedev_2\\Manage_Share\\Database_convert\\share_V2.sqlite')\n",
    "        cur = con.cursor()    \n",
    "        newshare = df.to_sql(table,con,if_exists='append', index=False)\n",
    "        con.commit()\n",
    "        con.close\n",
    "\n",
    "class find_location_share_News():\n",
    "\n",
    "    \n",
    "\n",
    "    def translate_text(self,text):\n",
    "\n",
    "        detector = Translator()\n",
    "\n",
    "        dec_lan = ''\n",
    "        for sec in range(int(len(text)/1000)+1):\n",
    "\n",
    "            dec_lan += detector.translate(text[1000*sec:1000*(sec+1)],des='en').text\n",
    "\n",
    "        return dec_lan\n",
    "\n",
    "\n",
    "    def Separate_words(self,text):\n",
    "\n",
    "        w = text.split(r'\\n')\n",
    "        for content in w : text+= content + ' '\n",
    "\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        # nlp = en_core_web_sm.load()\n",
    "        doc = nlp(text)\n",
    "\n",
    "        word = []\n",
    "\n",
    "        for token in doc.ents:\n",
    "            \n",
    "            if token.label_ != 'DATE' and token.label_ !='CARDINAL' and token.label_ !='TIME':\n",
    "                # print(token.text,token.label_)     \n",
    "                word += token.text.split('\"')\n",
    "\n",
    "        return list(numpy.unique(word))\n",
    "\n",
    "    def filter_special_2(self,words):\n",
    "        # Define regular expression pattern to match words\n",
    "        units,sequence,share_word = ['kg','kilograms',\"percent\",\"billion\",\"%\",\"megawatts\",'trillion','meters','#','million','cents'],['1st','2nd','3rd','first','second','third'],['Q1/','Q2/','Q3/','Q4/','SET','NASDAQ']\n",
    "        word_filter = []\n",
    "\n",
    "        # Find all matches of the pattern in the text\n",
    "        for word in words :\n",
    "            \n",
    "            if word not in sequence :\n",
    "\n",
    "                s = 0\n",
    "                \n",
    "                if \"%\" in word or '$' in word : s = 1\n",
    "\n",
    "                Subwords = word.split(' ')\n",
    "                \n",
    "                if len(Subwords) == 2 and s == 0  :\n",
    "                    for unit in units:\n",
    "                        if unit in Subwords : s = 1\n",
    "\n",
    "                if len(Subwords) == 1 and s == 0  :           \n",
    "                    for quart in share_word :\n",
    "                        if quart in word: s=1 \n",
    "\n",
    "                if isinstance(word, str) and word.isnumeric(): s = 1\n",
    "                    \n",
    "                if isinstance(word, str) and word.count('.') == 1 and all(char.isdigit() or char == '.' for char in word): s = 1\n",
    "                    \n",
    "                if s == 0: word_filter.append(word)\n",
    "                    \n",
    "        return list(numpy.unique(word_filter))\n",
    "\n",
    "    def search_share_in_table(self,words,NewsId,source):\n",
    "        # connect to the database\n",
    "        conn = sqlite3.connect(r'C:\\Users\\JourneyQ\\OneDrive - kmutnb.ac.th\\Desktop\\Quick_file\\year_2-S_2\\Project_sofedev_2\\Manage_Share\\Database_convert\\share_V2.sqlite')\n",
    "        c = conn.cursor()\n",
    "        share = []\n",
    "        loss = []\n",
    "\n",
    "        for word in words : \n",
    "\n",
    "            w = word\n",
    "            if source == 'kaohoon': w = word + '.BK'\n",
    "\n",
    "            # search for words in the table\n",
    "            c.execute(f\"\"\"SELECT * FROM Information WHERE Symbol=\"{w}\" or Sname=\"{word}\"; \"\"\")\n",
    "            results = c.fetchall()\n",
    "\n",
    "            if results != []:\n",
    "                data = {}\n",
    "                data['SymbolId'],data['NewsId'] = results[0][0],NewsId\n",
    "                share.append(data)\n",
    "\n",
    "            else :\n",
    "\n",
    "                loss.append(word)\n",
    "\n",
    "        # close the database connection\n",
    "        conn.close()\n",
    "\n",
    "        return share,loss\n",
    "\n",
    "    def find_location(self,words,NewsId):\n",
    "\n",
    "        geolocator = Nominatim(user_agent=\"Geolocation\")\n",
    "        loca,extra = [],[]\n",
    "        for i in words :\n",
    "    \n",
    "            location = geolocator.geocode(f\"{i}\", exactly_one=True, namedetails=True, addressdetails=True,timeout=12000, language='en')\n",
    "\n",
    "            if str(location) != 'None' :\n",
    "\n",
    "                data = {}\n",
    "\n",
    "                try :\n",
    "                    if i in ['Africa', 'Europe', 'Asia', 'North America', 'South America', 'Antarctica', 'Australia','South Pole','North pole'] :\n",
    "                        data['Lname'],data['Country'],data['point'],data['NewsId'] = i,'',f'({location.latitude}, {location.longitude})',NewsId\n",
    "                    else:\n",
    "                        data['Lname'],data['Country'],data['point'],data['NewsId'] = i,location.raw['address']['country'],f'({location.latitude}, {location.longitude})',NewsId\n",
    "\n",
    "                    loca.append(data)\n",
    "\n",
    "                except KeyError: extra.append(i)\n",
    "                    \n",
    "        return loca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
